---
title: CodToys - EP04ï¼šè«‹æ”¯æ´è¿‘ç¾©è©æœå°‹
description: å¿˜è¨˜é¡æ–‡å­—çš„ tag æ€éº¼è¾¦ï¼Ÿè«‹æ”¯æ´è¿‘ç¾©è©æœå°‹ áƒš(â•¹Îµâ•¹áƒš)
tags: ['Electron', 'LLM']
image: https://codlin.me/cod-toys.webp
date: 20250122
---

# CodToys - EP04ï¼šè«‹æ”¯æ´è¿‘ç¾©è©æœå°‹

![å¿«æ¨‚å †ç©æœ¨çš„é­š](/cod-toys.webp){.cover}

ä¸Šä¸€ç« æˆ‘å€‘å¯ä»¥å¾ˆå¿«é€Ÿåœ°é¸æ“‡ç›®æ¨™é¡æ–‡å­—äº†ã€‚

ä½†æ˜¯æ—¥å­ä¹…äº†ï¼Œå¯èƒ½æœƒå¿˜è¨˜é¡æ–‡å­—çš„ tagï¼Œå°è‡´ä¸€æ™‚ä¸çŸ¥é“æ€éº¼æ‰¾åˆ°æƒ³è¦çš„é¡æ–‡å­—ã€‚%(â—œà¯°â—)%

<br>

è·¯äººï¼šã€Œè‡ªå·±æ–°å¢çš„ tag é‚„èƒ½å¿˜è¨˜å–”ï¼Ÿ%(â—‰â—à±ªâ—Ÿâ—‰ )%ã€

é±ˆé­šï¼šã€Œæˆ‘å°±é±ˆé­šè…¦ï¼Œä½ æœ‰æ„è¦‹è†©ï¼Ÿ%(;Â´à¼àº¶Ğ”à¼àº¶`)%ã€

<br>

æ—¢ç„¶å¦‚æ­¤ï¼Œé‚£å°±å°±ä¾†æ”¯æ´è¿‘ç¾©è©æœå°‹å§ï¼Œé€™æ¨£å³ä½¿å¿˜è¨˜åŸæœ¬çš„ tagï¼Œä¹Ÿå¯ä»¥é€éç›¸è¿‘çš„å­—è©æ‰¾åˆ°æƒ³è¦çš„é¡æ–‡å­—äº†ï¼%ãƒ¾(â—'à±ª`â—)ï¾‰ï¾%

## é–‹ç™¼

ç›®å‰æƒ³åˆ°çš„å¯¦ä½œæ–¹å¼ç‚ºï¼šã€Œä½¿ç”¨ LLMï¼ˆLanguage Learning Modelï¼‰ä¾†å›ç­” tag çš„ä¸­è‹±æ–‡è¿‘ç¾©è©ï¼Œå†å°è¿‘ç¾©è©é€²è¡Œæ¨¡ç³ŠåŒ¹é…ã€

æœ¬ç« ç¯€é è¨ˆæœƒå»ºç«‹ä»¥ä¸‹å…§å®¹ï¼š

- åœ¨ Electron ä¸­é‹è¡Œ LLM
- è®“ LLM å°æ¯å€‹é¡æ–‡å­—çš„ tag ç”¢ç”Ÿè¿‘ç¾©è©
- æ ¹æ“šè¿‘ç¾©è©æœå°‹ä¸¦é¡¯ç¤ºçµæœ

### ä¾†é‹è¡Œ LLM å§ï¼

é€™è£¡ä¾è³´ [`node-llama-cpp`](https://github.com/withcatai/node-llama-cpp) é€™å€‹å¥—ä»¶ï¼Œè—‰ç”±æ­¤å¥—ä»¶æˆ‘å€‘å¯ä»¥å¾ˆç°¡å–®çš„åœ¨ Node.js ä¸­é‹è¡Œ LLMã€‚

é€™è£¡é¸ç”¨å°å‹çš„æ¨¡å‹ï¼ˆå¤§æ¨¡å‹ä¹Ÿè·‘ä¸å‹•å°±æ˜¯äº† (\Â´ãƒ»Ï‰ãƒ»\`)ï¼‰ï¼Œç›®å‰å¯¦éš›æ¸¬è©¦ `gemma-2-2b-it-Q6_K_L` å’Œ `Llama-3.2-3B-Instruct.Q6_K`ã€‚

ç¨‹å¼ç¢¼å¾ˆå–®ç´”ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯å¾®èª¿å®˜æ–¹ç¯„ä¾‹è€Œå·²ã€‚

è®“æ¨¡å‹å›ç­”ç‰¹å®šå–®å­—çš„ä¸­è‹±æ–‡è¿‘ç¾©è©ï¼Œä¸¦å°‡çµæœé¡¯ç¤ºå‡ºä¾†ã€‚

```typescript
import path from 'node:path'
import { fileURLToPath } from 'node:url'
import chalk from 'chalk'
import { getLlama, LlamaChatSession, resolveModelFile } from 'node-llama-cpp'

const __dirname = path.dirname(fileURLToPath(import.meta.url))
const modelsDirectory = path.join(__dirname, '..', 'models')

const llama = await getLlama()

const qList = [
  'å¿«æ¨‚çš„ 10 å€‹ç¹é«”ä¸­æ–‡ã€10 å€‹è‹±æ–‡è¿‘ç¾©è©ï¼Œå…¨éƒ¨åˆä½µï¼Œé€—è™Ÿåˆ†éš”ï¼Œä¸è¦ä»»ä½•é™„åŠ è³‡è¨Šï¼Œä¸è¦æ¨™é¡Œï¼Œä¸éœ€è§£é‡‹',
  'æ…¶ç¥çš„ 10 å€‹ç¹é«”ä¸­æ–‡ã€10 å€‹è‹±æ–‡è¿‘ç¾©è©ï¼Œå…¨éƒ¨åˆä½µï¼Œé€—è™Ÿåˆ†éš”ï¼Œä¸è¦ä»»ä½•é™„åŠ è³‡è¨Šï¼Œä¸è¦æ¨™é¡Œï¼Œä¸éœ€è§£é‡‹',
  'ç™¼ç˜‹çš„ 10 å€‹ç¹é«”ä¸­æ–‡ã€10 å€‹è‹±æ–‡è¿‘ç¾©è©ï¼Œå…¨éƒ¨åˆä½µï¼Œé€—è™Ÿåˆ†éš”ï¼Œä¸è¦ä»»ä½•é™„åŠ è³‡è¨Šï¼Œä¸è¦æ¨™é¡Œï¼Œä¸éœ€è§£é‡‹'
]

{
  console.log(chalk.yellow('gemma: '))

  const modelPath = await resolveModelFile(
    'hf_bartowski_gemma-2-2b-it-Q6_K_L.gguf',
    modelsDirectory
  )

  const model = await llama.loadModel({ modelPath })
  const context = await model.createContext()
  const session = new LlamaChatSession({
    contextSequence: context.getSequence()
  })
  console.log()

  for (const q of qList) {
    const a = await session.prompt(q)
    console.log(a)
  }

  await model.dispose()
}

{
  console.log(chalk.yellow('llama: '))

  const modelPath = await resolveModelFile(
    'hf_mradermacher_Llama-3.2-3B-Instruct.Q6_K.gguf',
    modelsDirectory
  )

  const model = await llama.loadModel({ modelPath })
  const context = await model.createContext()
  const session = new LlamaChatSession({
    contextSequence: context.getSequence()
  })
  console.log()

  for (const q of qList) {
    const a = await session.prompt(q)
    console.log(a)
  }

  await model.dispose()
}
```

ä»¥ä¸‹æ˜¯é‹è¡Œçµæœã€‚

gemma:

```text
å¹¸ç¦ã€æ¬£å–œã€æ„‰æ‚…ã€æ­¡æ¨‚ã€é–‹å¿ƒã€æ»¿è¶³ã€å–œæ‚…ã€èˆˆå¥®ã€æ»¿è¶³ã€èˆ’æš¢
joy, happiness, delight, exhilaration, contentment, pleasure, satisfaction, excitement, bliss, comfort

æ…¶ç¥ã€æ­¡æ…¶ã€æ…¶å…¸ã€ç››æœƒã€æ´¾å°ã€èšæœƒã€ç¯€æ…¶ã€ç´€å¿µã€å…¸ç¦®ã€å˜‰å¹´è¯
celebration, festivities, party, gathering, jubilee, festival, commemoration, gala, ceremony, parade, anniversary

ç™¼ç˜‹ã€ç˜‹ç‹‚ã€å¤±æ§ã€æ­‡æ–¯åº•é‡Œã€ç‹‚èºã€è¡å‹•ã€å¤±åºã€å¤±å¸¸ã€æš´èºã€èºç‹‚
madness, frenzy, unhinged, manic, agitated, impulsive, chaotic, disordered, irritable, erratic
```

llama:

```text
å¿«æ¨‚ã€æ„‰æ‚…ã€å¹¸ç¦ã€æ¬£å¿«ã€é«˜èˆˆã€å¿«æ„ã€æ‚…ç›®ã€æ„‰æ‚…ã€å¿«æ¨‚ã€å¿«æš¢

æ…¶ç¥ã€ç¥è³€ã€ç´€å¿µã€ç´€å¿µã€ç¥ç¦ã€æ…¶å…¸ã€ç¯€æ—¥ã€ç¯€æ…¶ã€ç´€å¿µã€ç¥è³€

ç™¼ç˜‹ã€ç™²ç™‡ã€ç˜‹ç‹‚ã€äº¢é€²ã€ç‹‚èºã€ç™¡å‘†ã€ç™²ç™‡ã€ç˜‹ç‹‚ã€äº¢é€²ã€ç‹‚èº
```

å¯ä»¥çœ‹å‡º gemma çš„å›ç­”å“è³ªæ¯” llama å¥½å¾ˆå¤šï¼Œæ‰€ä»¥æ¥ä¸‹ä¾†è®“æˆ‘å€‘æ¡ç”¨ gemma å¯¦ç¾è¿‘ç¾©è©æœå°‹å§ã€‚%( Â´ â–½ ` )ï¾‰%

### æ–°å¢ LLM API

å®‰è£ `node-llama-cpp` ä¹‹å¾Œï¼Œæ–°å¢ä¸€å€‹ preload çš„ APIï¼Œç”¨ä¾†åŸ·è¡Œ LLMã€‚

ä¸éåœ¨é–‹å§‹å‰ï¼Œæˆ‘å€‘ä¾†é‡æ§‹ä¸€ä¸‹ç›®å‰ `preload` çš„ç¨‹å¼ç¢¼ï¼Œè®“ `electron-env.d.ts` çš„å½¢åˆ¥è‡ªå‹•æ¨å°ï¼Œä¸è¦å†æ‰‹å‹•å®šç¾©ä¸€æ¬¡ã€‚

`electron\preload.ts`

```typescript
import type { UserConfig } from './electron-env'
import { contextBridge, ipcRenderer } from 'electron'

export const mainApi = {
  updateHeight(height: number) {
    return ipcRenderer.send('main:updateHeight', height)
  },
  hideWindow() {
    return ipcRenderer.send('main:hideWindow')
  },
  openExternal(url: string) {
    return ipcRenderer.send('main:openExternal', url)
  },
}
contextBridge.exposeInMainWorld('main', mainApi)

export const configApi = {
  get() {
    return ipcRenderer.invoke('config:get')
  },
  update(data: Partial<UserConfig>) {
    return ipcRenderer.invoke('config:update', data)
  },
  onUpdate(callback: (config: UserConfig) => void) {
    ipcRenderer.on('config:onUpdate', (event, config) => {
      callback(config)
    })
  },
}
contextBridge.exposeInMainWorld('config', configApi)
```

`electron-env.d` åªè¦å¼•å…¥ `preload` çš„ API å®šç¾©å³å¯ã€‚

`electron\electron-env.d.ts`

```typescript
import type { configApi, mainApi } from './preload'

export interface UserConfig {
  kaomoji: {
    databaseId: string;
    token: string;
  };
}

declare global {
  interface Window {
    main: typeof mainApi;
    config: typeof configApi;
  }
}

export { }
```

ç¾åœ¨ä¾†æ–°å¢ LLM APIã€‚

`electron\preload.ts`

```typescript
// ...

export const llmApi = {
  prompt(message: string): Promise<string> {
    return ipcRenderer.invoke('llm:prompt', message)
  },
}
contextBridge.exposeInMainWorld('llm', llmApi)
```

`electron\electron-env.d.ts`

```typescript
// ...

declare global {
  interface Window {
    main: typeof mainApi;
    config: typeof configApi;
    llm: typeof llmApi; // [!code ++]
  }
}

export { }
```

æ¥è‘—åœ¨ `main` ä¸­å¯¦ä½œ LLM çš„ APIã€‚

`electron\main.ts`

```typescript
// ...

async function initIpcMain(
  {
    configStore,
  }: {
    configStore: ConfigStore;
  },
) {
  // main
  ipcMain.on('main:updateHeight', (event, height: number) => {
    const window = BrowserWindow.fromWebContents(event.sender)

    window?.setBounds({ height })
  })
  ipcMain.on('main:hideWindow', (event) => {
    const window = BrowserWindow.fromWebContents(event.sender)

    window?.setFocusable(false)
    window?.hide()
  })
  ipcMain.on('main:openExternal', (event, url: string) => {
    shell.openExternal(url)
  })

  // config
  ipcMain.handle('config:get', async () => {
    return configStore.get('config')
  })
  ipcMain.handle('config:update', async (event, config) => {
    const data = configStore.get('config')
    configStore.set('config', {
      ...data,
      ...config,
    })

    // å‘æ‰€æœ‰è¦–çª—è§¸ç™¼ config:onUpdate äº‹ä»¶
    BrowserWindow.getAllWindows().forEach((window) => {
      window.webContents.send('config:onUpdate', config)
    })
  })

  // llm
  /**
   * node-llama-cpp åªæ”¯æ´ ESM
   *
   * ç›´æ¥ import æœƒå‡ºç¾ Error [ERR_REQUIRE_ESM]: require() of ES Module éŒ¯èª¤
   *
   * https://node-llama-cpp.withcat.ai/guide/troubleshooting#using-in-commonjs
   */
  const { getLlama, LlamaChatSession, resolveModelFile } = await import('node-llama-cpp')

  const modelPath = await resolveModelFile(
    'hf_bartowski_gemma-2-2b-it-Q6_K_L.gguf',
    modelsDirectory,
  )

  const llama = await getLlama()
  const model = await llama.loadModel({ modelPath })
  const context = await model.createContext()
  ipcMain.handle('llm:prompt', async (event, message: string) => {
    const session = new LlamaChatSession({
      contextSequence: context.getSequence(),
    })

    const answer = await session.prompt(message)
    session.dispose()

    return answer
  })
}

// ...

app.whenReady().then(async () => {
  const configStore = createConfigStore()
  await initIpcMain({ configStore })

  const mainWindow = await createInputWindow()

  initGlobalShortcut({ mainWindow })
  const tray = createTray({ mainWindow })

  app.on('window-all-closed', () => {
    if (process.platform !== 'darwin') {
      app.quit()
    }

    globalShortcut.unregisterAll()
    tray.destroy()
  })
})
```

ç¾åœ¨è®“æˆ‘å€‘ä¾†è©¦è©¦æ•ˆæœã€‚

`src\App.vue`

```vue
<template>
  {{ ans }}
  <router-view />
</template>

<script setup lang="ts">
import { ref } from 'vue'
import { useLlmApi } from './composables/use-llm-api'

// ...

const llmApi = useLlmApi()

const ans = ref('')
llmApi.prompt('ä½ å¥½ gemma').then((answer) => {
  ans.value = answer
})
</script>

// ...
```

![prompt-test](/cod-toys/prompt-test.png)

æœ‰äº†ï¼%( Â´ â–½ ` )ï¾‰%

`main.ts` çš„æ±è¥¿è¶Šä¾†è¶Šå¤šäº†ï¼Œç¾åœ¨è®“æˆ‘å€‘é‡æ§‹ä¸€ä¸‹ï¼Œå°‡å„è‡ªçš„åˆå§‹åŒ–ç¨‹å¼ç¢¼åˆ†é›¢ã€‚

## ç¸½çµ ğŸŸ

ä»¥ä¸Šç¨‹å¼ç¢¼å¯ä»¥[åœ¨æ­¤å–å¾—](https://github.com/Codfisher/side-project-cod-toys/tree/feat/kaomoji-add-llm)
